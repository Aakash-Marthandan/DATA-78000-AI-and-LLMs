{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Up6JwhKnqs5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read a file you have stored locally\n",
        "# I added the Hunger Games for simplicity\n",
        "file = open(\"hunger_games.txt\", 'r').read()\n",
        "\n",
        "# first, remove unwanted new line and tab characters from the text\n",
        "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
        "    file = file.replace(char, \" \")\n",
        "\n",
        "# check\n",
        "print(file[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtO8SRWVrQof",
        "outputId": "1cc251d6-03d5-4e3d-a3d4-e6f4508baf8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Second Book of THE HUNGER GAMES     New York Times Bestsel ling Author   SUZHNNE  COLLINS     PA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is simplified for demonstration\n",
        "def sample_clean_text(text: str):\n",
        "    # step 1: tokenize the text into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    # step 2: tokenize each sentence into words\n",
        "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # step 3: convert each word to lowercase\n",
        "    tokenized_text = [[word.lower() for word in sent] for sent in tokenized_sentences]\n",
        "\n",
        "    # return your tokens\n",
        "    return tokenized_text\n",
        "\n",
        "# call the function\n",
        "tokens = sample_clean_text(text = file)\n",
        "\n",
        "# check\n",
        "print(tokens[:10])"
      ],
      "metadata": {
        "id": "ES2Hymt0r7MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(tokens,vector_size=100)\n",
        "model.wv.get_vector(\"capitol\", norm=True)\n",
        "model.wv.most_similar('capitol')\n",
        "model.wv.similarity('katniss', 'girl')\n",
        "model.wv.similarity('peeta', 'home')\n",
        "\n",
        "def reduce_dimensions(model):\n",
        "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
        "\n",
        "    # extract the words & their vectors, as numpy arrays\n",
        "    vectors = np.asarray(model.wv.vectors)\n",
        "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
        "\n",
        "    # reduce using t-SNE\n",
        "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
        "    vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "    x_vals = [v[0] for v in vectors]\n",
        "    y_vals = [v[1] for v in vectors]\n",
        "    return x_vals, y_vals, labels\n",
        "\n",
        "\n",
        "x_vals, y_vals, labels = reduce_dimensions(model)\n"
      ],
      "metadata": {
        "id": "O_act0OftFTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
        "    from plotly.offline import init_notebook_mode, iplot, plot\n",
        "    import plotly.graph_objs as go\n",
        "\n",
        "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
        "    data = [trace]\n",
        "\n",
        "    if plot_in_notebook:\n",
        "        init_notebook_mode(connected=True)\n",
        "        iplot(data, filename='word-embedding-plot')\n",
        "    else:\n",
        "        plot(data, filename='word-embedding-plot.html')\n",
        "\n",
        "\n",
        "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import random\n",
        "\n",
        "    random.seed(0)\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.scatter(x_vals, y_vals)\n",
        "\n",
        "    #\n",
        "    # Label randomly subsampled 25 data points\n",
        "    #\n",
        "    indices = list(range(len(labels)))\n",
        "    selected_indices = random.sample(indices, 25)\n",
        "    for i in selected_indices:\n",
        "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
        "\n",
        "try:\n",
        "    get_ipython()\n",
        "except Exception:\n",
        "    plot_function = plot_with_matplotlib\n",
        "else:\n",
        "    plot_function = plot_with_plotly\n",
        "\n",
        "plot_function(x_vals, y_vals, labels)\n"
      ],
      "metadata": {
        "id": "9f5PNIkMtkbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#establish an empty dictionary\n",
        "embeddings_dict = {}\n",
        "\n",
        "#open the file and read it into the dictionary\n",
        "with open(\"glove.6B/glove.6B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector"
      ],
      "metadata": {
        "id": "hIZtOVbxt_bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find the Euclidean distance between the vectors for words and 1 or more other words.\n",
        "#sort the resulting word distances.\n",
        "def find_closest_embeddings(embedding):\n",
        "    return sorted(embeddings_dict.keys(), key=lambda word:\n",
        "                  spatial.distance.euclidean(embeddings_dict[word], embedding))\n",
        "\n",
        "print(find_closest_embeddings(embeddings_dict[\"dog\"])[:20])\n",
        "print(find_closest_embeddings(embeddings_dict[\"cat\"])[:20])\n",
        "print(find_closest_embeddings(embeddings_dict[\"dog\"] + embeddings_dict[\"cat\"])[:20])\n",
        "print(find_closest_embeddings(embeddings_dict[\"dog\"] + embeddings_dict[\"cat\"] + embeddings_dict[\"pet\"])[:20])"
      ],
      "metadata": {
        "id": "TwMeSE1ot_fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words =  list(embeddings_dict.keys())\n",
        "vectors = [embeddings_dict[word] for word in words]\n",
        "X = np.asarray(vectors)\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "Y = tsne.fit_transform(X[:1000])\n",
        "plt.scatter(Y[:, 0], Y[:, 1])\n",
        "\n",
        "for label, x, y in zip(words, X[:, 0], Y[:, 1]):\n",
        "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\"offset points\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "do26RRdSt_i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kEFebrbLt_lz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}